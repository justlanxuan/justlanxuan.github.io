<!DOCTYPE html><!--z8GRT5LGDjWLqL24KDRTZ--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/avatar.jpg"/><link rel="stylesheet" href="/_next/static/css/0e220de1137f06af.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-74c939c87fa0092a.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-77a2a609117b40ff.js" async=""></script><script src="/_next/static/chunks/main-app-19723b242f09e660.js" async=""></script><script src="/_next/static/chunks/619-ba102abea3e3d0e4.js" async=""></script><script src="/_next/static/chunks/239-c2117d3ee033cea8.js" async=""></script><script src="/_next/static/chunks/app/layout-364510cdeed0ce2c.js" async=""></script><script src="/_next/static/chunks/app/posts/%5Bslug%5D/page-40506bc87a3f15c3.js" async=""></script><title>Lanxuan</title><meta name="description" content="Lanxuan&#x27;s personal website"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><header class="navbar"><nav class="nav-container"><a class="logo-container" href="/"><div class="logo-avatar"><img alt="Lanxuan avatar" decoding="async" data-nimg="fill" class="avatar" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/avatar.jpg"/></div><span class="logo">Lanxuan</span></a><ul class="nav-menu desktop-menu"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/projects">Projects</a></li><li><a href="/courses">Courses</a></li><li><a href="/posts">Posts</a></li></ul><button class="mobile-toggle" aria-label="Toggle navigation">☰</button></nav></header><main><div class="page-layout"><div class="page-content"><article class="post-wrapper"><header><h1 class="post-title">ID Reconstruction via IMU and Camera</h1><p class="post-date">2025-11-12</p></header><section class="post-content"><h2>Background</h2>
<p>In the context of Autism Spectrum Disorder (ASD) early screening, one key challenge is to <strong>track</strong> each individual child&#39;s behavior over time. In our specified configuration, each child wears a smart wristband that captures bio-signals related to health, with a camera system installed in the room to capture behavioral data visually. The altimate goal is to combine these 2 modalities for Autism detection.  </p>
<p>Accurate analysis relies on the ability to maintain consistent identity information over time: it&#39;s important to know which behavior and bio-signal (straightforward, since everyone wears the same wristband everyday) belong to which person. It&#39;s meaningless to analyze the whole video as a single entity. A crutial step to achieve <strong>ID alignment</strong>:</p>
<ul>
<li>Maintain consistent ID tracking in a crowded environment with multiple children;</li>
<li>Matching visual detections (children observed in the video) with the corresponding wearable sensor data;</li>
</ul>
<p>This also helps to improve the granularity of behavior analysis, as different behaviors may be better captured in different modalities.</p>
<h2>Problem Statement</h2>
<p>The objective is to jointly analyze and align the <strong>video data (visual modality)</strong> and <strong>wearable sensor data (physiological modality)</strong> so as to <strong>consistently identify and track each individual child across time and multiple viewpoints</strong>.</p>
<figure>
  <img src="/post/reid1.png" alt="ReID via IMU and Camera" class="small">
  <figcaption>AI generated scene for this specification</figcaption>
</figure>

<h2>Potential Strategies</h2>
<p>Several possible strategies was considered through paper review and my experiments:</p>
<ol>
<li><strong>ReID task via visual data only.</strong> This is from the intuition that every person has unique visual features (e.g., face, body shape, clothing) that can be used for identification. In our case, the kids change clothes daily, making it challenging to rely solely on visual features. Also, it only aligns the behaviors detected via camera, without considering the wearable biosignal data.</li>
<li><strong>Indoor localization.</strong>, i.e., tracking each kid&#39;s location via WiFi/BLE signals from their smart watches, and maps to their distance to the camera. This can provide a rough alignment between the wearable data and the visual data, but may not be precise enough for individual identification, especially in crowded scenes. UWB-based localization has higher accuracy, but requires additional infrastructure. <span style="color:rgb(150, 0, 0);"><strong>Note: this ould narrow down the search space.</strong></span></li>
<li><strong>Event triggered alignment.</strong> Identify specific events (e.g., sudden movements, falls) in both visual and wearable data, and use these events as anchor points for alignment. This could help the synchronization, but may not cover all behaviors. <span style="color:rgb(150, 0, 0);"><strong>An idea from this is, what if I trace all the events with a low-power IMU embedded in the smart watch, and align all behaviors instead of specific events?</strong></span></li>
</ol>
<p>My high-level idea is based on the following intuition:</p>
<ol>
<li><p>Each person carries an IMU sensor (in the smart wristband) that captures motion data. We can reconstruct the <strong>hand trajectory</strong> and <strong>hand pose</strong> from the IMU data.</p>
</li>
<li><p>The <strong>hand trajectory</strong> and <strong>hand pose</strong> can be extracted from the video data using computer vision techniques, possibly with different granularities.</p>
</li>
<li><p>Each person should have completely different trajectory over a period of time. By comparing the reconstructed trajectories and poses from both modalities, we can align the visual detections with the corresponding wearable sensor data.</p>
</li>
</ol>
<p>Things to experiment with:</p>
<ol>
<li>IMU-based hand trajectory reconstruction: Extended Kalman Filter, Complementary Filter, or deep learning-based methods.</li>
<li>IMU-based gesture estimation:</li>
</ol>
<h2>Archeived Trials</h2>
<ul>
<li>Directly compute the acceleration from skeleton keypoints extracted from video, and compare with IMU data. <span style="color:rgb(150, 0, 0);"><strong>maybe possible with long-term tracking</strong></span></li>
</ul>
<figure>
  <img src="/post/reid2.png" alt="Differentiating the skeleton position extracted from video">
  <figcaption>Differentiating the skeleton position extracted from video</figcaption>
</figure>
- A better way to encode IMU data and skeleton data using deep learning, and learn a shared embedding space for alignment.
<figure>
  <img src="/post/reid3.png" alt="Learning a shared embedding space for alignment">
  <figcaption>Learning a shared embedding space for alignment</figcaption>
</figure></section><nav class="post-nav"><a class="prev-post" href="/posts/icra-2025">← <!-- -->ICRA2025</a><a class="next-post" href="/posts/research-update">Research Updates<!-- --> →</a></nav></article></div></div><!--$--><!--/$--></main><footer class="site-footer"><div>© 2025 Lanxuan</div><div>Last updated on <!-- -->Sep 2025</div></footer><script src="/_next/static/chunks/webpack-74c939c87fa0092a.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[8358,[\"619\",\"static/chunks/619-ba102abea3e3d0e4.js\",\"239\",\"static/chunks/239-c2117d3ee033cea8.js\",\"177\",\"static/chunks/app/layout-364510cdeed0ce2c.js\"],\"default\"]\n3:I[9766,[],\"\"]\n4:I[8924,[],\"\"]\n6:I[4431,[],\"OutletBoundary\"]\n8:I[5278,[],\"AsyncMetadataOutlet\"]\na:I[4431,[],\"ViewportBoundary\"]\nc:I[4431,[],\"MetadataBoundary\"]\nd:\"$Sreact.suspense\"\nf:I[7150,[],\"\"]\n:HL[\"/_next/static/css/0e220de1137f06af.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"z8GRT5LGDjWLqL24KDRTZ\",\"p\":\"\",\"c\":[\"\",\"posts\",\"reid\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"reid\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/0e220de1137f06af.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"site-footer\",\"children\":[[\"$\",\"div\",null,{\"children\":\"© 2025 Lanxuan\"}],[\"$\",\"div\",null,{\"children\":[\"Last updated on \",\"Sep 2025\"]}]]}]]}]}]]}],{\"children\":[\"posts\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"reid\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"promise\":\"$@9\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],null],[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$d\",null,{\"fallback\":null,\"children\":\"$Le\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:T133a,"])</script><script>self.__next_f.push([1,"\u003ch2\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn the context of Autism Spectrum Disorder (ASD) early screening, one key challenge is to \u003cstrong\u003etrack\u003c/strong\u003e each individual child\u0026#39;s behavior over time. In our specified configuration, each child wears a smart wristband that captures bio-signals related to health, with a camera system installed in the room to capture behavioral data visually. The altimate goal is to combine these 2 modalities for Autism detection.  \u003c/p\u003e\n\u003cp\u003eAccurate analysis relies on the ability to maintain consistent identity information over time: it\u0026#39;s important to know which behavior and bio-signal (straightforward, since everyone wears the same wristband everyday) belong to which person. It\u0026#39;s meaningless to analyze the whole video as a single entity. A crutial step to achieve \u003cstrong\u003eID alignment\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMaintain consistent ID tracking in a crowded environment with multiple children;\u003c/li\u003e\n\u003cli\u003eMatching visual detections (children observed in the video) with the corresponding wearable sensor data;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis also helps to improve the granularity of behavior analysis, as different behaviors may be better captured in different modalities.\u003c/p\u003e\n\u003ch2\u003eProblem Statement\u003c/h2\u003e\n\u003cp\u003eThe objective is to jointly analyze and align the \u003cstrong\u003evideo data (visual modality)\u003c/strong\u003e and \u003cstrong\u003ewearable sensor data (physiological modality)\u003c/strong\u003e so as to \u003cstrong\u003econsistently identify and track each individual child across time and multiple viewpoints\u003c/strong\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg src=\"/post/reid1.png\" alt=\"ReID via IMU and Camera\" class=\"small\"\u003e\n  \u003cfigcaption\u003eAI generated scene for this specification\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch2\u003ePotential Strategies\u003c/h2\u003e\n\u003cp\u003eSeveral possible strategies was considered through paper review and my experiments:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eReID task via visual data only.\u003c/strong\u003e This is from the intuition that every person has unique visual features (e.g., face, body shape, clothing) that can be used for identification. In our case, the kids change clothes daily, making it challenging to rely solely on visual features. Also, it only aligns the behaviors detected via camera, without considering the wearable biosignal data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndoor localization.\u003c/strong\u003e, i.e., tracking each kid\u0026#39;s location via WiFi/BLE signals from their smart watches, and maps to their distance to the camera. This can provide a rough alignment between the wearable data and the visual data, but may not be precise enough for individual identification, especially in crowded scenes. UWB-based localization has higher accuracy, but requires additional infrastructure. \u003cspan style=\"color:rgb(150, 0, 0);\"\u003e\u003cstrong\u003eNote: this ould narrow down the search space.\u003c/strong\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvent triggered alignment.\u003c/strong\u003e Identify specific events (e.g., sudden movements, falls) in both visual and wearable data, and use these events as anchor points for alignment. This could help the synchronization, but may not cover all behaviors. \u003cspan style=\"color:rgb(150, 0, 0);\"\u003e\u003cstrong\u003eAn idea from this is, what if I trace all the events with a low-power IMU embedded in the smart watch, and align all behaviors instead of specific events?\u003c/strong\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMy high-level idea is based on the following intuition:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eEach person carries an IMU sensor (in the smart wristband) that captures motion data. We can reconstruct the \u003cstrong\u003ehand trajectory\u003c/strong\u003e and \u003cstrong\u003ehand pose\u003c/strong\u003e from the IMU data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe \u003cstrong\u003ehand trajectory\u003c/strong\u003e and \u003cstrong\u003ehand pose\u003c/strong\u003e can be extracted from the video data using computer vision techniques, possibly with different granularities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eEach person should have completely different trajectory over a period of time. By comparing the reconstructed trajectories and poses from both modalities, we can align the visual detections with the corresponding wearable sensor data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThings to experiment with:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIMU-based hand trajectory reconstruction: Extended Kalman Filter, Complementary Filter, or deep learning-based methods.\u003c/li\u003e\n\u003cli\u003eIMU-based gesture estimation:\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eArcheived Trials\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDirectly compute the acceleration from skeleton keypoints extracted from video, and compare with IMU data. \u003cspan style=\"color:rgb(150, 0, 0);\"\u003e\u003cstrong\u003emaybe possible with long-term tracking\u003c/strong\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\n  \u003cimg src=\"/post/reid2.png\" alt=\"Differentiating the skeleton position extracted from video\"\u003e\n  \u003cfigcaption\u003eDifferentiating the skeleton position extracted from video\u003c/figcaption\u003e\n\u003c/figure\u003e\n- A better way to encode IMU data and skeleton data using deep learning, and learn a shared embedding space for alignment.\n\u003cfigure\u003e\n  \u003cimg src=\"/post/reid3.png\" alt=\"Learning a shared embedding space for alignment\"\u003e\n  \u003cfigcaption\u003eLearning a shared embedding space for alignment\u003c/figcaption\u003e\n\u003c/figure\u003e"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"page-layout\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"page-content\",\"children\":[\"$\",\"article\",null,{\"className\":\"post-wrapper\",\"children\":[[\"$\",\"header\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"post-title\",\"children\":\"ID Reconstruction via IMU and Camera\"}],[\"$\",\"p\",null,{\"className\":\"post-date\",\"children\":\"2025-11-12\"}]]}],[\"$\",\"section\",null,{\"className\":\"post-content\",\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}],\"$L11\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"12:I[2619,[\"619\",\"static/chunks/619-ba102abea3e3d0e4.js\",\"858\",\"static/chunks/app/posts/%5Bslug%5D/page-40506bc87a3f15c3.js\"],\"\"]\n11:[\"$\",\"nav\",null,{\"className\":\"post-nav\",\"children\":[[\"$\",\"$L12\",null,{\"href\":\"/posts/icra-2025\",\"className\":\"prev-post\",\"children\":[\"← \",\"ICRA2025\"]}],[\"$\",\"$L12\",null,{\"href\":\"/posts/research-update\",\"className\":\"next-post\",\"children\":[\"Research Updates\",\" →\"]}]]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"13:I[622,[],\"IconMark\"]\n9:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Lanxuan\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Lanxuan's personal website\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"$L13\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"e:\"$9:metadata\"\n"])</script></body></html>