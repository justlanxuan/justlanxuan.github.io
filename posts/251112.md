---
title: "ID Reconstruction via IMU and Camera"
date: "2025-11-12"
slug: "reid"
excerpt: "Some ideas from recent papers"
---
## Background
In the context of Autism Spectrum Disorder (ASD) early screening, one key challenge is to **track** each individual child's behavior over time. In our specified configuration, each child wears a smart wristband that captures bio-signals related to health, with a camera system installed in the room to capture behavioral data visually. The altimate goal is to combine these 2 modalities for Autism detection.  

Accurate analysis relies on the ability to maintain consistent identity information over time: it's important to know which behavior and bio-signal (straightforward, since everyone wears the same wristband everyday) belong to which person. It's meaningless to analyze the whole video as a single entity. A crutial step to achieve **ID alignment**:
- Maintain consistent ID tracking in a crowded environment with multiple children;
- Matching visual detections (children observed in the video) with the corresponding wearable sensor data;

This also helps to improve the granularity of behavior analysis, as different behaviors may be better captured in different modalities.

## Problem Statement
The objective is to jointly analyze and align the **video data (visual modality)** and **wearable sensor data (physiological modality)** so as to **consistently identify and track each individual child across time and multiple viewpoints**.

<figure>
  <img src="/post/reid1.png" alt="ReID via IMU and Camera" class="small">
  <figcaption>AI generated scene for this specification</figcaption>
</figure>

## Potential Strategies
Several possible strategies was considered through paper review and my experiments:

1. **ReID task via visual data only.** This is from the intuition that every person has unique visual features (e.g., face, body shape, clothing) that can be used for identification. In our case, the kids change clothes daily, making it challenging to rely solely on visual features. Also, it only aligns the behaviors detected via camera, without considering the wearable biosignal data.
2. **Indoor localization.**, i.e., tracking each kid's location via WiFi/BLE signals from their smart watches, and maps to their distance to the camera. This can provide a rough alignment between the wearable data and the visual data, but may not be precise enough for individual identification, especially in crowded scenes. UWB-based localization has higher accuracy, but requires additional infrastructure. <span style="color:rgb(150, 0, 0);">**Note: this ould narrow down the search space.**</span>
3. **Event triggered alignment.** Identify specific events (e.g., sudden movements, falls) in both visual and wearable data, and use these events as anchor points for alignment. This could help the synchronization, but may not cover all behaviors. <span style="color:rgb(150, 0, 0);">**An idea from this is, what if I trace all the events with a low-power IMU embedded in the smart watch, and align all behaviors instead of specific events?**</span>

My high-level idea is based on the following intuition:
1. Each person carries an IMU sensor (in the smart wristband) that captures motion data. We can reconstruct the **hand trajectory** and **hand pose** from the IMU data.

2. The **hand trajectory** and **hand pose** can be extracted from the video data using computer vision techniques, possibly with different granularities.

3. Each person should have completely different trajectory over a period of time. By comparing the reconstructed trajectories and poses from both modalities, we can align the visual detections with the corresponding wearable sensor data.

Things to experiment with:
1. IMU-based hand trajectory reconstruction: Extended Kalman Filter, Complementary Filter, or deep learning-based methods.
2. IMU-based gesture estimation:

## Archeived Trials
- Directly compute the acceleration from skeleton keypoints extracted from video, and compare with IMU data. <span style="color:rgb(150, 0, 0);">**maybe possible with long-term tracking**</span>
<figure>
  <img src="/post/reid2.png" alt="Differentiating the skeleton position extracted from video">
  <figcaption>Differentiating the skeleton position extracted from video</figcaption>
</figure>
- A better way to encode IMU data and skeleton data using deep learning, and learn a shared embedding space for alignment.
<figure>
  <img src="/post/reid3.png" alt="Learning a shared embedding space for alignment">
  <figcaption>Learning a shared embedding space for alignment</figcaption>
</figure>